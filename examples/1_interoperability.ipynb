{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ea8bd70",
   "metadata": {},
   "source": [
    "# Interoperability\n",
    "\n",
    "The core tenant of InteropRouter is to allow for seamless interoperability between AI providers.\n",
    "\n",
    "InteropRouter is based on the [OpenAI Responses API](https://platform.openai.com/docs/api-reference/responses) as its common interface as it supports the most features across providers. See [Migrate to the Responses API](https://platform.openai.com/docs/guides/migrate-to-responses) for more details.\n",
    "\n",
    "The `Router` is the central abstraction. You register provider clients once, then make calls using a unified API. The router dispatches to the correct provider based on the model name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ddabb6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T22:10:49.850129Z",
     "iopub.status.busy": "2025-12-23T22:10:49.850023Z",
     "iopub.status.idle": "2025-12-23T22:10:50.486288Z",
     "shell.execute_reply": "2025-12-23T22:10:50.485286Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from anthropic import AsyncAnthropic\n",
    "from google import genai\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "from interop_router.router import Router\n",
    "from interop_router.types import RouterResponse\n",
    "\n",
    "router = Router()\n",
    "openai_client = AsyncOpenAI()\n",
    "gemini_client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "anthropic_client = AsyncAnthropic()\n",
    "router.register(\"openai\", openai_client)\n",
    "router.register(\"gemini\", gemini_client)\n",
    "router.register(\"anthropic\", anthropic_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16fc20c",
   "metadata": {},
   "source": [
    "## Strong Typing\n",
    "\n",
    "InteropRouter expects you to use OpenAI Responses API types directly. For example, use `EasyInputMessageParam` from `openai.types.responses` to construct simple user and assistant messages.\n",
    "\n",
    "These types are wrapped in `ChatMessage` for flexibility to store data necessary for interoperability, then translated to each provider's native format by the router.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a8649e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T22:10:50.487575Z",
     "iopub.status.busy": "2025-12-23T22:10:50.487385Z",
     "iopub.status.idle": "2025-12-23T22:10:50.490599Z",
     "shell.execute_reply": "2025-12-23T22:10:50.489725Z"
    }
   },
   "outputs": [],
   "source": [
    "from openai.types.responses import EasyInputMessageParam\n",
    "\n",
    "from interop_router.types import ChatMessage\n",
    "\n",
    "messages: list[ChatMessage] = []\n",
    "user_message = \"\"\"Classify the text into one of the provided categories and return the answer into the following JSON format: {\"category\": \"CATEGORY\"}\n",
    "TEXT: Samba 3.8B, a simple Mamba+Sliding Window Attention architecture. And it has an infinite context length with linear complexity.\n",
    "CATEGORIES: Unsupervised Learning, Statistics, SLM, LLM, Computer Vision, Reinforcement Learning\n",
    "Think hard, but only return the JSON object as specified above.\"\"\"\n",
    "messages.append(ChatMessage(message=EasyInputMessageParam(role=\"user\", content=user_message)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe95f1d",
   "metadata": {},
   "source": [
    "## Optimal Translation\n",
    "\n",
    "InteropRouter automatically translates parameters seamlessly between providers.\n",
    "\n",
    "For example, we can use the OpenAI reasoning settings, but that will be translated to the corresponding parameter for Gemini and Anthropic. This allows you to use nearly the same code and your knowledge of the Responses API across providers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "356d2472",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T22:10:50.491698Z",
     "iopub.status.busy": "2025-12-23T22:10:50.491594Z",
     "iopub.status.idle": "2025-12-23T22:11:05.516906Z",
     "shell.execute_reply": "2025-12-23T22:11:05.515937Z"
    }
   },
   "outputs": [],
   "source": [
    "openai_response = await router.create(\n",
    "    input=messages,\n",
    "    model=\"gpt-5.2\",\n",
    "    reasoning={\"effort\": \"medium\", \"summary\": \"auto\"},\n",
    "    include=[\"reasoning.encrypted_content\"],\n",
    "    max_output_tokens=120_000,\n",
    ")\n",
    "\n",
    "gemini_response = await router.create(\n",
    "    input=messages,\n",
    "    model=\"gemini-3-flash-preview\",\n",
    "    reasoning={\"effort\": \"low\", \"summary\": \"auto\"},  # This maps to high reasoning effort in Gemini\n",
    "    include=[\n",
    "        \"reasoning.encrypted_content\"\n",
    "    ],  # This sets include_thoughts=True in the Gemini ThinkingConfig which is necessary to get reasoning content\n",
    "    max_output_tokens=120_000,\n",
    ")\n",
    "\n",
    "anthropic_response = await router.create(\n",
    "    input=messages,\n",
    "    model=\"claude-haiku-4-5-20251001\",\n",
    "    reasoning={\"effort\": \"medium\", \"summary\": \"auto\"},\n",
    "    include=[\"reasoning.encrypted_content\"],\n",
    "    max_output_tokens=64_000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b3c393",
   "metadata": {},
   "source": [
    "## Response Structure\n",
    "\n",
    "`RouterResponse.output` contains a list of `ChatMessage` objects with provider-normalized content. Since InteropRouter converts all provider responses to the OpenAI Responses API format, you can use the same extraction logic regardless of which provider generated the response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4eb2235",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T22:11:05.518126Z",
     "iopub.status.busy": "2025-12-23T22:11:05.518006Z",
     "iopub.status.idle": "2025-12-23T22:11:05.522359Z",
     "shell.execute_reply": "2025-12-23T22:11:05.521371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1: {\"category\":\"SLM\"}\n",
      "Model 2: {\"category\": \"SLM\"}\n",
      "Model 3: ```json\n",
      "{\"category\": \"SLM\"}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "def extract_text(response: RouterResponse) -> str:\n",
    "    texts: list[str] = []\n",
    "    for chat_message in response.output:\n",
    "        if chat_message.message.get(\"type\") == \"message\":\n",
    "            content = chat_message.message.get(\"content\")\n",
    "            if isinstance(content, list):\n",
    "                for c in content:\n",
    "                    text = c.get(\"text\", \"\")\n",
    "                    if text:\n",
    "                        texts.append(text)\n",
    "    return \"\\n\".join(texts)\n",
    "\n",
    "\n",
    "generated_text = (\n",
    "    \"Model 1: \"\n",
    "    + extract_text(openai_response)\n",
    "    + \"\\nModel 2: \"\n",
    "    + extract_text(gemini_response)\n",
    "    + \"\\nModel 3: \"\n",
    "    + extract_text(anthropic_response)\n",
    ")\n",
    "print(generated_text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fdb7043",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T22:11:05.523521Z",
     "iopub.status.busy": "2025-12-23T22:11:05.523413Z",
     "iopub.status.idle": "2025-12-23T22:11:16.624266Z",
     "shell.execute_reply": "2025-12-23T22:11:16.623147Z"
    }
   },
   "outputs": [],
   "source": [
    "user_message_2 = (\n",
    "    \"These are three different model responses:\\n\"\n",
    "    + generated_text.strip()\n",
    "    + \"\\nPlease make a final determination of the category.\"\n",
    ")\n",
    "\n",
    "assistant_message = EasyInputMessageParam(\n",
    "    role=\"user\",\n",
    "    content=user_message_2,\n",
    "    type=\"message\",\n",
    ")\n",
    "messages.append(ChatMessage(message=assistant_message))\n",
    "\n",
    "gemini_response_2 = await router.create(\n",
    "    input=messages,\n",
    "    model=\"gemini-3-pro-preview\",\n",
    "    reasoning={\"effort\": \"high\", \"summary\": \"auto\"},\n",
    "    include=[\"reasoning.encrypted_content\"],\n",
    "    max_output_tokens=120_000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dd3f0cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T22:11:16.625497Z",
     "iopub.status.busy": "2025-12-23T22:11:16.625387Z",
     "iopub.status.idle": "2025-12-23T22:11:16.628685Z",
     "shell.execute_reply": "2025-12-23T22:11:16.627713Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REASONING SUMMARY:\n",
      "**Thinking Through the Classification**\n",
      "\n",
      "Okay, so I've got this text snippet: \"Samba 3.8B, a simple Mamba+Sliding Window Attention architecture. And it has an infinite context length with linear complexity.\"  My goal is to classify it into one of these six categories: Unsupervised Learning, Statistics, SLM, LLM, Computer Vision, or Reinforcement Learning.\n",
      "\n",
      "First, I need to break down the text. \"Samba 3.8B\" screams AI model to me, with 3.8B suggesting size – likely parameters. \"Mamba+Sliding Window Attention\" strongly points towards language modeling, and \"infinite context length with linear complexity\" adds to this picture, alluding to efficient processing of long sequences.\n",
      "\n",
      "Now, let's analyze the categories. Unsupervised Learning is a *method* of training, not the model itself, so that's out. Statistics is a field of math, again, not what this describes. Computer Vision is for images/video; Mamba is for sequences - so not that. Reinforcement Learning is about agents and rewards, which also doesn't fit.\n",
      "\n",
      "That leaves SLM and LLM. Historically, \"LLM\" could easily encompass a 3.8B model, but the field is evolving. \"SLM\" has become common for smaller models, often under 7B or 10B parameters, to highlight their efficiency.  Microsoft's Phi models are a good example.  The very mention of \"Samba 3.8B\" suggests an emphasis on a compact, efficient architecture, which makes SLM the better fit *given the options provided*. If the list was just LLM as the only language model category, then LLM would be the answer, but since SLM is there, it's more specific.\n",
      "\n",
      "Looking at the example answers, I see that they all picked \"SLM\", confirming my thought process. The model responded with `{\"category\":\"SLM\"}`.\n",
      "\n",
      "So, I'm confident my final answer should be: `{\"category\": \"SLM\"}`. This directly addresses the prompt's request for a JSON object.\n",
      "RESPONSE:\n",
      "{\"category\": \"SLM\"}\n"
     ]
    }
   ],
   "source": [
    "# Get the reasoning summary\n",
    "summary = \"\"\n",
    "for message in gemini_response_2.output:\n",
    "    if message.message.get(\"type\") == \"reasoning\":\n",
    "        for summary_content in message.message.get(\"summary\", []):\n",
    "            summary += summary_content.get(\"text\", \"\")\n",
    "\n",
    "print(\"REASONING SUMMARY:\\n\" + summary.strip())\n",
    "print(\"RESPONSE:\\n\" + extract_text(gemini_response_2).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebfd2c7",
   "metadata": {},
   "source": [
    "## Easy Conversations\n",
    "\n",
    "Each `ChatMessage` in `RouterResponse.output` has its `.message` field typed as `ResponseInputItemParam`. This means you can directly extend your conversation history with previous responses. Just append the messages from the previous response `messages.extend(response.output)` to continue the conversation.\n",
    "\n",
    "The format was also designed to make it easy to serialize and deserialize to and from JSON for storage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e1a4076",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T22:11:16.629623Z",
     "iopub.status.busy": "2025-12-23T22:11:16.629527Z",
     "iopub.status.idle": "2025-12-23T22:11:16.633456Z",
     "shell.execute_reply": "2025-12-23T22:11:16.632691Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(message={'role': 'user', 'content': 'Classify the text into one of the provided categories and return the answer into the following JSON format: {\"category\": \"CATEGORY\"}\\nTEXT: Samba 3.8B, a simple Mamba+Sliding Window Attention architecture. And it has an infinite context length with linear complexity.\\nCATEGORIES: Unsupervised Learning, Statistics, SLM, LLM, Computer Vision, Reinforcement Learning\\nThink hard, but only return the JSON object as specified above.'}, id='8eb90298-fe8b-4eb9-987a-3b2f25c7f04f', timestamp=datetime.datetime(2025, 12, 23, 22, 10, 50, 488445, tzinfo=datetime.timezone.utc), created_by='user', interop={}, metadata={}, provider_kwargs={}, original_response=None),\n",
       " ChatMessage(message={'role': 'user', 'content': 'These are three different model responses:\\nModel 1: {\"category\":\"SLM\"}\\nModel 2: {\"category\": \"SLM\"}\\nModel 3: ```json\\n{\"category\": \"SLM\"}\\n```\\nPlease make a final determination of the category.', 'type': 'message'}, id='a8e37550-35b2-467f-a4da-9a594b030346', timestamp=datetime.datetime(2025, 12, 23, 22, 11, 5, 524169, tzinfo=datetime.timezone.utc), created_by='user', interop={}, metadata={}, provider_kwargs={}, original_response=None),\n",
       " ChatMessage(message={'id': '401b3064-ac6f-4f3e-bf30-a7446ffce927', 'type': 'reasoning', 'summary': [{'text': '**Thinking Through the Classification**\\n\\nOkay, so I\\'ve got this text snippet: \"Samba 3.8B, a simple Mamba+Sliding Window Attention architecture. And it has an infinite context length with linear complexity.\"  My goal is to classify it into one of these six categories: Unsupervised Learning, Statistics, SLM, LLM, Computer Vision, or Reinforcement Learning.\\n\\nFirst, I need to break down the text. \"Samba 3.8B\" screams AI model to me, with 3.8B suggesting size – likely parameters. \"Mamba+Sliding Window Attention\" strongly points towards language modeling, and \"infinite context length with linear complexity\" adds to this picture, alluding to efficient processing of long sequences.\\n\\nNow, let\\'s analyze the categories. Unsupervised Learning is a *method* of training, not the model itself, so that\\'s out. Statistics is a field of math, again, not what this describes. Computer Vision is for images/video; Mamba is for sequences - so not that. Reinforcement Learning is about agents and rewards, which also doesn\\'t fit.\\n\\nThat leaves SLM and LLM. Historically, \"LLM\" could easily encompass a 3.8B model, but the field is evolving. \"SLM\" has become common for smaller models, often under 7B or 10B parameters, to highlight their efficiency.  Microsoft\\'s Phi models are a good example.  The very mention of \"Samba 3.8B\" suggests an emphasis on a compact, efficient architecture, which makes SLM the better fit *given the options provided*. If the list was just LLM as the only language model category, then LLM would be the answer, but since SLM is there, it\\'s more specific.\\n\\nLooking at the example answers, I see that they all picked \"SLM\", confirming my thought process. The model responded with `{\"category\":\"SLM\"}`.\\n\\nSo, I\\'m confident my final answer should be: `{\"category\": \"SLM\"}`. This directly addresses the prompt\\'s request for a JSON object.\\n\\n\\n', 'type': 'summary_text'}], 'status': 'completed', 'encrypted_content': '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'}, id='cc183321-4b03-4975-b639-b7e51d07881d', timestamp=datetime.datetime(2025, 12, 23, 22, 11, 16, 621875, tzinfo=datetime.timezone.utc), created_by='gemini', interop={}, metadata={}, provider_kwargs={}, original_response=None),\n",
       " ChatMessage(message={'id': '98bc6389-d9d4-470e-a291-260b305bd792', 'content': [{'text': '{\"category\": \"SLM\"}', 'type': 'output_text', 'annotations': []}], 'role': 'assistant', 'status': 'completed', 'type': 'message'}, id='df1d5746-fefa-49d2-be7a-895f7c966161', timestamp=datetime.datetime(2025, 12, 23, 22, 11, 16, 621886, tzinfo=datetime.timezone.utc), created_by='gemini', interop={}, metadata={}, provider_kwargs={}, original_response={'sdk_http_response': {'headers': {'content-type': 'application/json; charset=UTF-8', 'vary': 'Origin, X-Origin, Referer', 'content-encoding': 'gzip', 'date': 'Tue, 23 Dec 2025 22:11:16 GMT', 'server': 'scaffolding on HTTPServer2', 'x-xss-protection': '0', 'x-frame-options': 'SAMEORIGIN', 'x-content-type-options': 'nosniff', 'server-timing': 'gfet4t7; dur=11629', 'alt-svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000', 'transfer-encoding': 'chunked'}, 'body': None}, 'candidates': [{'content': {'parts': [{'media_resolution': None, 'code_execution_result': None, 'executable_code': None, 'file_data': None, 'function_call': None, 'function_response': None, 'inline_data': None, 'text': '**Thinking Through the Classification**\\n\\nOkay, so I\\'ve got this text snippet: \"Samba 3.8B, a simple Mamba+Sliding Window Attention architecture. And it has an infinite context length with linear complexity.\"  My goal is to classify it into one of these six categories: Unsupervised Learning, Statistics, SLM, LLM, Computer Vision, or Reinforcement Learning.\\n\\nFirst, I need to break down the text. \"Samba 3.8B\" screams AI model to me, with 3.8B suggesting size – likely parameters. \"Mamba+Sliding Window Attention\" strongly points towards language modeling, and \"infinite context length with linear complexity\" adds to this picture, alluding to efficient processing of long sequences.\\n\\nNow, let\\'s analyze the categories. Unsupervised Learning is a *method* of training, not the model itself, so that\\'s out. Statistics is a field of math, again, not what this describes. Computer Vision is for images/video; Mamba is for sequences - so not that. Reinforcement Learning is about agents and rewards, which also doesn\\'t fit.\\n\\nThat leaves SLM and LLM. Historically, \"LLM\" could easily encompass a 3.8B model, but the field is evolving. \"SLM\" has become common for smaller models, often under 7B or 10B parameters, to highlight their efficiency.  Microsoft\\'s Phi models are a good example.  The very mention of \"Samba 3.8B\" suggests an emphasis on a compact, efficient architecture, which makes SLM the better fit *given the options provided*. If the list was just LLM as the only language model category, then LLM would be the answer, but since SLM is there, it\\'s more specific.\\n\\nLooking at the example answers, I see that they all picked \"SLM\", confirming my thought process. The model responded with `{\"category\":\"SLM\"}`.\\n\\nSo, I\\'m confident my final answer should be: `{\"category\": \"SLM\"}`. This directly addresses the prompt\\'s request for a JSON object.\\n\\n\\n', 'thought': True, 'thought_signature': None, 'video_metadata': None}, {'media_resolution': None, 'code_execution_result': None, 'executable_code': None, 'file_data': None, 'function_call': None, 'function_response': None, 'inline_data': None, 'text': '{\"category\": \"SLM\"}', 'thought': None, 'thought_signature': 'EoYaCoMaAXLI2nzOma1_PW637-QfzPbjV5Zb2SVwEcb5E-hcwDbLBTVB6SPCgCzdQxvFnHcwp2J1sc4qecKGmZQxAvG1jjAkHub3FhLyUXwr42JWSek3leZW6rtZ4NEvWUP1V5xXjJp9G9ORZ_3n3PKCgu3v5ATLblwbyG7PPGREiT6WEsywoMfoBpLul0UJ7un-tcFERwAAJ5QbkNQ_4k-7jjE3Gctb7tDzN8Tc2fHVXnxXiei4Zao3fh0LT357UiRzBJ10-YRWDUUmlITFT-mUb2rNfZ1r_k_RxW_iT-ObrvmvClv9TtCy3viRO5a9_SwzXV9iWQ5pdj3HwvfhgXgQV6D0_15USx-Hp1U0qCpXb38iVq9L45p8czizXflxA2nqZbAMjmlrezgm8dcYAJOt2vxvl89edIHqlBk7_8Ph_wBQHPkDv4D2azvpuOqKMN2o2wVkx5oWtDkfUnvjIXpfR14Ihk5GGsyrskmXA-k8J-Djt33CQ_Xm451-Rcc7EcXF3pZWw5ALx_T3Owctp2XzRf8cju0NHocHe5yXJfplkSkWWd-IaPjbNEWL1c6EOBqfaFm2LMc8L7PKkamKbOfYnnnT8ddW0qN8JOZCa4fv-cZfikwAmq1VMTiUz_Ybegit_AoCnmq-tG-mfCYv-Bg98Q39yFEqbKqcTbLooLEo8CHbcEvVJsV2rDVrb8BI5MHSrEG6cIkX7aQ804NPq1fZhwv8djIwQykcNuSa4nielE1FbBfLrFKBdE2iWtr8v8HXz-_hnaH3WlgC7XAXln0ZNUcv3IfRVsYOveYMppSX-JcMnNVP5s7k8hhKiOiY-vpmkonuqmSrunvFEw05cnawtMmKYEJY_WzyxGIW_osulq58_hexkhch7FQ5rtIzBLDfvJQ32g7ZqFeaZmHNvGOzjlLJptPeREbk2C_JlJBp0U26qTgxpbQn6nb5gYOal-4nlOHfGNWYh4dIiBiEN6M1ffb9q6A_RFcih7NawyW0Bo7F-sq5NkFR2pEwYeFLv82bMcC7QFyjWD0OTdtIhMErigYPig8QHW87yu6Jyk01TyVB850l_FvvDQl6SSqRt5kISjdnFhR8G4eEfDMptqi73ot0oNgH7DIdxF_OQDPC-OmaPduvxceUlJ9EXQRNObuXwgb1MafTQs41G6NDo4x4eJSc_BFKZYfWzEh_tHmlR84oBJQmzwxEx7Mhckfinmz2FNa4qj0MfFlRVEH6lkp6yQ3pSCcYddBEGVll83WZfIK2saDsmvaSnSLKMsii7Zh8pyLS6NeFkHZ0B48uohKxwW3JkOfIOfS3RPj9jXrqiqqRtcPJbT69v4PKn1MtHX49zvU6_Ji1LxC69ICT7OcTjlc0UndntOWpPh8p2qPGf1K0wyw9CIIGzm5SwMiZvOyY0rvddR987FTfNOFFgQlvq8JnAcZO18gYIe6ou7pAeN1DUbRqO7iTivOmNJjDYWxuU4nflvtwv_UdkGynN1GBlrG2skY_tM0P3TcV67zDvf3GSKp3lgp9VQE-e4z1Da3FVBqtB4AR3GVGuzxh2pqF39AQdvzBVnV_hm5zsPQcdApBcH3cEZV_zZZShDa0IRuc_hrpsqNpI1q5vmShxpNEfDGqE0Ne2ZuUr26xxwmey4MXeCCN6z9l6bHkwDuiEza1rXuYvdjdjTGFObvWiGx7U9MFg1xp-5f_xmmqjIxsvc7slmBVXLZNEMmLmPAj4jigakG6ptG1E8V7h-vGQ3LNMqiTR1yFXNS2BSH1Qb5oQe9KWa73yRGKiBu02IlnDw85qM4ZkHPnK2oOoVx9f3sKjceRnqxskPyIiqFaZk9siFrQzTYLWgEIEK9mSh5e-9irkNqEg1SvJ0s8GGI8XJvIn69CHK4ggmVONOhF-Z0uDsnuXR7Ix7ns-Z0146WsR_8Vhz42mDg_cEqie7FlWmaPhPnSM4BIg6YMaDn1ZsnIuliWu9L2i4vTgZy74hkI4XqmROITwqjwzzfWbQHbGXpzF59r2zkvsXF3Xc_JKnH_0mQuZF-nKO6_tMDWBSEvM0QNujnsY9Fu_YzQLWVte1syQuEVvRosg7miL0aJiPxsHoG5W4LOsYtTTOxldNJ6_IzlPoU6rBuG7dePUAhKUkfvq9astDKQGJNVTTL84WPaqS2grEO_e7kyfCZ_VkMXhvITXuJKEdA6KJj-jOlTw-CIepuLOEyF2pkA9O03gwfm_Jj5GUIE3cKjtLa7EB9dXuHP6IrT_rryPcfCQT9LMmC6_-GkG6p0LvajeKUAeu8qsxY155fpwHU3WoI7KNs3X57j-qFDOKVe-pQZB41d6WjaMWxgh6RCBLncXid7pS4YItWpV9pFHzMVQuRGggK3y5d1TD6PmdKL4nlO8UcIetOma1VRwL3YOuoKBy98XrXVUG3PR8uTnNF7YwIWsNM2xRM7-yH_rvkiWcK0PiD5mdNsxaTkvqBWdtDckGYZAs7DGBx5WZhaxQNDmn3NAT7lqiWRqg2DmPfM6ueTyaJK0ffl1mNRWav7v5eapnO3CgUEsmyELHsys2mI-WbsO_nFc4UNOWKOOutnoe7iZcj3j3drBX2DAHV5f-ksb8ETJkcBRB1oh2sxnkSSL4U1k3XyiCAdja7-3cbHbjAqWwBPUwA0VIoVdMtdYcymecArlw93HuA__spehTFCOtnhdJtbjXNTExztbJxNnssIQHvOogVktO2uoiyZ8BhBDGKfqEIDvs1oG2AuXuAHcOzSo3ZbP3M2czC6VgLVz5qT1rkin2hDqTLbERMwmYzfDJIg5O_teLQN7HhCdPIypyQmFIeBoUGPYI9tj4Y84K82yg94JD3RV1jj1EqCESPB89_zYcZRijJPZBCx2j6Vk7hGXS9dQ1oYt0sAq2a7PoQQgb7Sd6ofr2-zOgDPYM6xidFRa9F9UtfAonx_wEB5A2q-5HHVirnAwpKEbLMcXZpWpPjfhLYMUbVNKAffcT9wNOep9LNIUe9bz_Fqz1KMU9ZQToy_2fm0Fr55hoo5sOh1On9SQwWxYqkU4zFKo6itnn-n30z76nRIrg8iadJ9cI0D_mNGNFAmDs29J9wNAHBad9Yggl-wZ8Gcd2Vi7iPn-Wf8eWGM-ar9nzQ1Adefng5YWvv0MVzuYbzOv3SVxCn-4JXb3lrFaWkzaqVHY3OAI1GJR0hwTiQiMYXZR7-oqbBuEcIssM4Fgwt6XsMa3YpnuKRZBuA3y3etzCxZdKtxc1TNbT2Lv_-6tHHztQkyhPvayNpAS9b9Il91ovI1MEvWLUEmk8woAOeYnv_ezFPqjnXQTLnZDwMgEle1-LCkurknHEyps31hhg5x-oK170QLuU9TmUcf6rAHkwS7xsp8Vyc_gXH4r-M2tT8JhIilXu0-3R6GQHpxMviCxyzC0458uCGsaGt3blGxDNp80Zt85RZFGKABofkEmigPXVjyMwXkg6HyHRqhdU3lZR1TR3VyU8yII8bohsjhhHZRAsuFDlUVcOOhz2e-yvEqnn0R29bp6iw-hqVr_dq0pX3dsjqaU7DyiVpHeLP7bwK4Onh_TvjhhuhFH9lyb8D71T_hf85DFKs3xk8zQ6kialThviGYnBKJKPBhrkM9vX03aispwjvdsU7JG_lK1J_GWK8jdcdM2fs2x3FbLvTSiFpKGFUnyzublr6VOdm1KRG2vtc1ZSQY-CmnwQEBvyaaVPYB2N5mVXWmvrwWy6ns5qaxoWYYD6TEbO_j5DVgFOgTPDlvs86ceFcRL3Fs_SXKg78iK6ikw_ope1l7kC-F632L3dCrzXtv7z_Zdalb9X6A6_WrTvXVoLnDTvAQxPTby-i-H0X73VtU8JozGzE6O0cIAMPN0Q2oMFKdv9OkhiISsDB1Hgqy56KofsTialvI2_zgbzxSuyl-QCahHZbK6rp1RTRt5NJkeGU5BJO-fMat8MGCuruDO5Wg_F-pXtGn5ylplf-Kd-ALbAgO5ch949bTTNMV9V8aZXrWQfQV-f5jnw87OeKdsbqqk8JhLGq9LV4a6Om-p_8VXjJgZk32EcZ1RjOkKvzPclyCCXcINZKXd5XJtvElDvPht5qfA33e-PiboWptCba8LL-TGhyFu2tY1x-UwxLaYh_Ou4SJBEZMmuXDuugyZtaPdDaRyHz8XZNB4HLB39xAt4VGtMO7YmDU1e611TAZAxHlTeFULy1FnH5bqFRh7zQAnc8FY9TAyGFYVmEi6HWEBLweIIb6vJdp1lgx293WlEPqBF3VcfdnTKJhf8en7Fax-qwBhvlf8WyLfp6q1UtOaP2lCEKmQHDmT25D7a0-xJH8oDeNmbDZYdbmsQaeHsf8pLjkK8QeojCppe8NmWIUwVhJHl2Gvbs6I3uEDz06XZvPlcSuCIxS7aE_LGMCekGDSq_Eg9AQ-U3ZZePCaKEgNWAasp0C3oAZzbye4lClIz92e36pDdGRzQ==', 'video_metadata': None}], 'role': 'model'}, 'citation_metadata': None, 'finish_message': None, 'token_count': None, 'finish_reason': 'STOP', 'avg_logprobs': None, 'grounding_metadata': None, 'index': 0, 'logprobs_result': None, 'safety_ratings': None, 'url_context_metadata': None}], 'create_time': None, 'model_version': 'gemini-3-pro-preview', 'prompt_feedback': None, 'response_id': 'hBNLaYbQFbafjrEPp-30wAs', 'usage_metadata': {'cache_tokens_details': None, 'cached_content_token_count': None, 'candidates_token_count': 7, 'candidates_tokens_details': None, 'prompt_token_count': 153, 'prompt_tokens_details': [{'modality': 'TEXT', 'token_count': 153}], 'thoughts_token_count': 815, 'tool_use_prompt_token_count': None, 'tool_use_prompt_tokens_details': None, 'total_token_count': 975, 'traffic_type': None}, 'automatic_function_calling_history': [], 'parsed': None})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.extend(gemini_response_2.output)\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "838cd420",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T22:11:16.634422Z",
     "iopub.status.busy": "2025-12-23T22:11:16.634324Z",
     "iopub.status.idle": "2025-12-23T22:11:21.510592Z",
     "shell.execute_reply": "2025-12-23T22:11:21.509662Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESPONSE:\n",
      "SLMs use fewer parameters and simplified or efficiency‑oriented mechanisms (like sparse or windowed attention) to run on limited hardware, whereas LLMs stack more layers and full self‑attention to maximize capacity and generality.\n"
     ]
    }
   ],
   "source": [
    "user_message_2 = \"\"\"How does the architecture of SLMs typically differ from LLMs? Keep it brief.\"\"\"\n",
    "messages.append(ChatMessage(message=EasyInputMessageParam(role=\"user\", content=user_message_2)))\n",
    "openai_response_2 = await router.create(\n",
    "    input=messages,\n",
    "    model=\"gpt-5.1-codex-max\",\n",
    "    reasoning={\"effort\": \"medium\", \"summary\": \"auto\"},\n",
    "    include=[\"reasoning.encrypted_content\"],\n",
    "    max_output_tokens=120_000,\n",
    ")\n",
    "print(\"RESPONSE:\\n\" + extract_text(openai_response_2).strip())\n",
    "messages.extend(openai_response_2.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73aec6f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T22:11:21.511932Z",
     "iopub.status.busy": "2025-12-23T22:11:21.511800Z",
     "iopub.status.idle": "2025-12-23T22:11:21.516791Z",
     "shell.execute_reply": "2025-12-23T22:11:21.515840Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27483"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the conversation to a file\n",
    "from datetime import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "tmp_dir = Path.cwd() / \"tmp\" / \"conversations\"\n",
    "tmp_dir.mkdir(parents=True, exist_ok=True)\n",
    "chat_file = tmp_dir / f\"chat_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "\n",
    "chat_file.write_text(json.dumps([json.loads(m.model_dump_json()) for m in messages], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5u3psi5ebdq",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T22:11:21.517860Z",
     "iopub.status.busy": "2025-12-23T22:11:21.517735Z",
     "iopub.status.idle": "2025-12-23T22:11:21.521814Z",
     "shell.execute_reply": "2025-12-23T22:11:21.520990Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(message={'role': 'user', 'content': 'Classify the text into one of the provided categories and return the answer into the following JSON format: {\"category\": \"CATEGORY\"}\\nTEXT: Samba 3.8B, a simple Mamba+Sliding Window Attention architecture. And it has an infinite context length with linear complexity.\\nCATEGORIES: Unsupervised Learning, Statistics, SLM, LLM, Computer Vision, Reinforcement Learning\\nThink hard, but only return the JSON object as specified above.'}, id='8eb90298-fe8b-4eb9-987a-3b2f25c7f04f', timestamp=datetime.datetime(2025, 12, 23, 22, 10, 50, 488445, tzinfo=datetime.timezone.utc), created_by='user', interop={}, metadata={}, provider_kwargs={}, original_response=None),\n",
       " ChatMessage(message={'role': 'user', 'content': 'These are three different model responses:\\nModel 1: {\"category\":\"SLM\"}\\nModel 2: {\"category\": \"SLM\"}\\nModel 3: ```json\\n{\"category\": \"SLM\"}\\n```\\nPlease make a final determination of the category.', 'type': 'message'}, id='a8e37550-35b2-467f-a4da-9a594b030346', timestamp=datetime.datetime(2025, 12, 23, 22, 11, 5, 524169, tzinfo=datetime.timezone.utc), created_by='user', interop={}, metadata={}, provider_kwargs={}, original_response=None),\n",
       " ChatMessage(message={'id': '401b3064-ac6f-4f3e-bf30-a7446ffce927', 'type': 'reasoning', 'summary': [{'text': '**Thinking Through the Classification**\\n\\nOkay, so I\\'ve got this text snippet: \"Samba 3.8B, a simple Mamba+Sliding Window Attention architecture. And it has an infinite context length with linear complexity.\"  My goal is to classify it into one of these six categories: Unsupervised Learning, Statistics, SLM, LLM, Computer Vision, or Reinforcement Learning.\\n\\nFirst, I need to break down the text. \"Samba 3.8B\" screams AI model to me, with 3.8B suggesting size – likely parameters. \"Mamba+Sliding Window Attention\" strongly points towards language modeling, and \"infinite context length with linear complexity\" adds to this picture, alluding to efficient processing of long sequences.\\n\\nNow, let\\'s analyze the categories. Unsupervised Learning is a *method* of training, not the model itself, so that\\'s out. Statistics is a field of math, again, not what this describes. Computer Vision is for images/video; Mamba is for sequences - so not that. Reinforcement Learning is about agents and rewards, which also doesn\\'t fit.\\n\\nThat leaves SLM and LLM. Historically, \"LLM\" could easily encompass a 3.8B model, but the field is evolving. \"SLM\" has become common for smaller models, often under 7B or 10B parameters, to highlight their efficiency.  Microsoft\\'s Phi models are a good example.  The very mention of \"Samba 3.8B\" suggests an emphasis on a compact, efficient architecture, which makes SLM the better fit *given the options provided*. If the list was just LLM as the only language model category, then LLM would be the answer, but since SLM is there, it\\'s more specific.\\n\\nLooking at the example answers, I see that they all picked \"SLM\", confirming my thought process. The model responded with `{\"category\":\"SLM\"}`.\\n\\nSo, I\\'m confident my final answer should be: `{\"category\": \"SLM\"}`. This directly addresses the prompt\\'s request for a JSON object.\\n\\n\\n', 'type': 'summary_text'}], 'status': 'completed', 'encrypted_content': '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'}, id='cc183321-4b03-4975-b639-b7e51d07881d', timestamp=datetime.datetime(2025, 12, 23, 22, 11, 16, 621875, tzinfo=datetime.timezone.utc), created_by='gemini', interop={}, metadata={}, provider_kwargs={}, original_response=None),\n",
       " ChatMessage(message={'id': '98bc6389-d9d4-470e-a291-260b305bd792', 'content': [{'text': '{\"category\": \"SLM\"}', 'type': 'output_text', 'annotations': []}], 'role': 'assistant', 'status': 'completed', 'type': 'message'}, id='df1d5746-fefa-49d2-be7a-895f7c966161', timestamp=datetime.datetime(2025, 12, 23, 22, 11, 16, 621886, tzinfo=datetime.timezone.utc), created_by='gemini', interop={}, metadata={}, provider_kwargs={}, original_response={'sdk_http_response': {'headers': {'content-type': 'application/json; charset=UTF-8', 'vary': 'Origin, X-Origin, Referer', 'content-encoding': 'gzip', 'date': 'Tue, 23 Dec 2025 22:11:16 GMT', 'server': 'scaffolding on HTTPServer2', 'x-xss-protection': '0', 'x-frame-options': 'SAMEORIGIN', 'x-content-type-options': 'nosniff', 'server-timing': 'gfet4t7; dur=11629', 'alt-svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000', 'transfer-encoding': 'chunked'}, 'body': None}, 'candidates': [{'content': {'parts': [{'media_resolution': None, 'code_execution_result': None, 'executable_code': None, 'file_data': None, 'function_call': None, 'function_response': None, 'inline_data': None, 'text': '**Thinking Through the Classification**\\n\\nOkay, so I\\'ve got this text snippet: \"Samba 3.8B, a simple Mamba+Sliding Window Attention architecture. And it has an infinite context length with linear complexity.\"  My goal is to classify it into one of these six categories: Unsupervised Learning, Statistics, SLM, LLM, Computer Vision, or Reinforcement Learning.\\n\\nFirst, I need to break down the text. \"Samba 3.8B\" screams AI model to me, with 3.8B suggesting size – likely parameters. \"Mamba+Sliding Window Attention\" strongly points towards language modeling, and \"infinite context length with linear complexity\" adds to this picture, alluding to efficient processing of long sequences.\\n\\nNow, let\\'s analyze the categories. Unsupervised Learning is a *method* of training, not the model itself, so that\\'s out. Statistics is a field of math, again, not what this describes. Computer Vision is for images/video; Mamba is for sequences - so not that. Reinforcement Learning is about agents and rewards, which also doesn\\'t fit.\\n\\nThat leaves SLM and LLM. Historically, \"LLM\" could easily encompass a 3.8B model, but the field is evolving. \"SLM\" has become common for smaller models, often under 7B or 10B parameters, to highlight their efficiency.  Microsoft\\'s Phi models are a good example.  The very mention of \"Samba 3.8B\" suggests an emphasis on a compact, efficient architecture, which makes SLM the better fit *given the options provided*. If the list was just LLM as the only language model category, then LLM would be the answer, but since SLM is there, it\\'s more specific.\\n\\nLooking at the example answers, I see that they all picked \"SLM\", confirming my thought process. The model responded with `{\"category\":\"SLM\"}`.\\n\\nSo, I\\'m confident my final answer should be: `{\"category\": \"SLM\"}`. This directly addresses the prompt\\'s request for a JSON object.\\n\\n\\n', 'thought': True, 'thought_signature': None, 'video_metadata': None}, {'media_resolution': None, 'code_execution_result': None, 'executable_code': None, 'file_data': None, 'function_call': None, 'function_response': None, 'inline_data': None, 'text': '{\"category\": \"SLM\"}', 'thought': None, 'thought_signature': 'EoYaCoMaAXLI2nzOma1_PW637-QfzPbjV5Zb2SVwEcb5E-hcwDbLBTVB6SPCgCzdQxvFnHcwp2J1sc4qecKGmZQxAvG1jjAkHub3FhLyUXwr42JWSek3leZW6rtZ4NEvWUP1V5xXjJp9G9ORZ_3n3PKCgu3v5ATLblwbyG7PPGREiT6WEsywoMfoBpLul0UJ7un-tcFERwAAJ5QbkNQ_4k-7jjE3Gctb7tDzN8Tc2fHVXnxXiei4Zao3fh0LT357UiRzBJ10-YRWDUUmlITFT-mUb2rNfZ1r_k_RxW_iT-ObrvmvClv9TtCy3viRO5a9_SwzXV9iWQ5pdj3HwvfhgXgQV6D0_15USx-Hp1U0qCpXb38iVq9L45p8czizXflxA2nqZbAMjmlrezgm8dcYAJOt2vxvl89edIHqlBk7_8Ph_wBQHPkDv4D2azvpuOqKMN2o2wVkx5oWtDkfUnvjIXpfR14Ihk5GGsyrskmXA-k8J-Djt33CQ_Xm451-Rcc7EcXF3pZWw5ALx_T3Owctp2XzRf8cju0NHocHe5yXJfplkSkWWd-IaPjbNEWL1c6EOBqfaFm2LMc8L7PKkamKbOfYnnnT8ddW0qN8JOZCa4fv-cZfikwAmq1VMTiUz_Ybegit_AoCnmq-tG-mfCYv-Bg98Q39yFEqbKqcTbLooLEo8CHbcEvVJsV2rDVrb8BI5MHSrEG6cIkX7aQ804NPq1fZhwv8djIwQykcNuSa4nielE1FbBfLrFKBdE2iWtr8v8HXz-_hnaH3WlgC7XAXln0ZNUcv3IfRVsYOveYMppSX-JcMnNVP5s7k8hhKiOiY-vpmkonuqmSrunvFEw05cnawtMmKYEJY_WzyxGIW_osulq58_hexkhch7FQ5rtIzBLDfvJQ32g7ZqFeaZmHNvGOzjlLJptPeREbk2C_JlJBp0U26qTgxpbQn6nb5gYOal-4nlOHfGNWYh4dIiBiEN6M1ffb9q6A_RFcih7NawyW0Bo7F-sq5NkFR2pEwYeFLv82bMcC7QFyjWD0OTdtIhMErigYPig8QHW87yu6Jyk01TyVB850l_FvvDQl6SSqRt5kISjdnFhR8G4eEfDMptqi73ot0oNgH7DIdxF_OQDPC-OmaPduvxceUlJ9EXQRNObuXwgb1MafTQs41G6NDo4x4eJSc_BFKZYfWzEh_tHmlR84oBJQmzwxEx7Mhckfinmz2FNa4qj0MfFlRVEH6lkp6yQ3pSCcYddBEGVll83WZfIK2saDsmvaSnSLKMsii7Zh8pyLS6NeFkHZ0B48uohKxwW3JkOfIOfS3RPj9jXrqiqqRtcPJbT69v4PKn1MtHX49zvU6_Ji1LxC69ICT7OcTjlc0UndntOWpPh8p2qPGf1K0wyw9CIIGzm5SwMiZvOyY0rvddR987FTfNOFFgQlvq8JnAcZO18gYIe6ou7pAeN1DUbRqO7iTivOmNJjDYWxuU4nflvtwv_UdkGynN1GBlrG2skY_tM0P3TcV67zDvf3GSKp3lgp9VQE-e4z1Da3FVBqtB4AR3GVGuzxh2pqF39AQdvzBVnV_hm5zsPQcdApBcH3cEZV_zZZShDa0IRuc_hrpsqNpI1q5vmShxpNEfDGqE0Ne2ZuUr26xxwmey4MXeCCN6z9l6bHkwDuiEza1rXuYvdjdjTGFObvWiGx7U9MFg1xp-5f_xmmqjIxsvc7slmBVXLZNEMmLmPAj4jigakG6ptG1E8V7h-vGQ3LNMqiTR1yFXNS2BSH1Qb5oQe9KWa73yRGKiBu02IlnDw85qM4ZkHPnK2oOoVx9f3sKjceRnqxskPyIiqFaZk9siFrQzTYLWgEIEK9mSh5e-9irkNqEg1SvJ0s8GGI8XJvIn69CHK4ggmVONOhF-Z0uDsnuXR7Ix7ns-Z0146WsR_8Vhz42mDg_cEqie7FlWmaPhPnSM4BIg6YMaDn1ZsnIuliWu9L2i4vTgZy74hkI4XqmROITwqjwzzfWbQHbGXpzF59r2zkvsXF3Xc_JKnH_0mQuZF-nKO6_tMDWBSEvM0QNujnsY9Fu_YzQLWVte1syQuEVvRosg7miL0aJiPxsHoG5W4LOsYtTTOxldNJ6_IzlPoU6rBuG7dePUAhKUkfvq9astDKQGJNVTTL84WPaqS2grEO_e7kyfCZ_VkMXhvITXuJKEdA6KJj-jOlTw-CIepuLOEyF2pkA9O03gwfm_Jj5GUIE3cKjtLa7EB9dXuHP6IrT_rryPcfCQT9LMmC6_-GkG6p0LvajeKUAeu8qsxY155fpwHU3WoI7KNs3X57j-qFDOKVe-pQZB41d6WjaMWxgh6RCBLncXid7pS4YItWpV9pFHzMVQuRGggK3y5d1TD6PmdKL4nlO8UcIetOma1VRwL3YOuoKBy98XrXVUG3PR8uTnNF7YwIWsNM2xRM7-yH_rvkiWcK0PiD5mdNsxaTkvqBWdtDckGYZAs7DGBx5WZhaxQNDmn3NAT7lqiWRqg2DmPfM6ueTyaJK0ffl1mNRWav7v5eapnO3CgUEsmyELHsys2mI-WbsO_nFc4UNOWKOOutnoe7iZcj3j3drBX2DAHV5f-ksb8ETJkcBRB1oh2sxnkSSL4U1k3XyiCAdja7-3cbHbjAqWwBPUwA0VIoVdMtdYcymecArlw93HuA__spehTFCOtnhdJtbjXNTExztbJxNnssIQHvOogVktO2uoiyZ8BhBDGKfqEIDvs1oG2AuXuAHcOzSo3ZbP3M2czC6VgLVz5qT1rkin2hDqTLbERMwmYzfDJIg5O_teLQN7HhCdPIypyQmFIeBoUGPYI9tj4Y84K82yg94JD3RV1jj1EqCESPB89_zYcZRijJPZBCx2j6Vk7hGXS9dQ1oYt0sAq2a7PoQQgb7Sd6ofr2-zOgDPYM6xidFRa9F9UtfAonx_wEB5A2q-5HHVirnAwpKEbLMcXZpWpPjfhLYMUbVNKAffcT9wNOep9LNIUe9bz_Fqz1KMU9ZQToy_2fm0Fr55hoo5sOh1On9SQwWxYqkU4zFKo6itnn-n30z76nRIrg8iadJ9cI0D_mNGNFAmDs29J9wNAHBad9Yggl-wZ8Gcd2Vi7iPn-Wf8eWGM-ar9nzQ1Adefng5YWvv0MVzuYbzOv3SVxCn-4JXb3lrFaWkzaqVHY3OAI1GJR0hwTiQiMYXZR7-oqbBuEcIssM4Fgwt6XsMa3YpnuKRZBuA3y3etzCxZdKtxc1TNbT2Lv_-6tHHztQkyhPvayNpAS9b9Il91ovI1MEvWLUEmk8woAOeYnv_ezFPqjnXQTLnZDwMgEle1-LCkurknHEyps31hhg5x-oK170QLuU9TmUcf6rAHkwS7xsp8Vyc_gXH4r-M2tT8JhIilXu0-3R6GQHpxMviCxyzC0458uCGsaGt3blGxDNp80Zt85RZFGKABofkEmigPXVjyMwXkg6HyHRqhdU3lZR1TR3VyU8yII8bohsjhhHZRAsuFDlUVcOOhz2e-yvEqnn0R29bp6iw-hqVr_dq0pX3dsjqaU7DyiVpHeLP7bwK4Onh_TvjhhuhFH9lyb8D71T_hf85DFKs3xk8zQ6kialThviGYnBKJKPBhrkM9vX03aispwjvdsU7JG_lK1J_GWK8jdcdM2fs2x3FbLvTSiFpKGFUnyzublr6VOdm1KRG2vtc1ZSQY-CmnwQEBvyaaVPYB2N5mVXWmvrwWy6ns5qaxoWYYD6TEbO_j5DVgFOgTPDlvs86ceFcRL3Fs_SXKg78iK6ikw_ope1l7kC-F632L3dCrzXtv7z_Zdalb9X6A6_WrTvXVoLnDTvAQxPTby-i-H0X73VtU8JozGzE6O0cIAMPN0Q2oMFKdv9OkhiISsDB1Hgqy56KofsTialvI2_zgbzxSuyl-QCahHZbK6rp1RTRt5NJkeGU5BJO-fMat8MGCuruDO5Wg_F-pXtGn5ylplf-Kd-ALbAgO5ch949bTTNMV9V8aZXrWQfQV-f5jnw87OeKdsbqqk8JhLGq9LV4a6Om-p_8VXjJgZk32EcZ1RjOkKvzPclyCCXcINZKXd5XJtvElDvPht5qfA33e-PiboWptCba8LL-TGhyFu2tY1x-UwxLaYh_Ou4SJBEZMmuXDuugyZtaPdDaRyHz8XZNB4HLB39xAt4VGtMO7YmDU1e611TAZAxHlTeFULy1FnH5bqFRh7zQAnc8FY9TAyGFYVmEi6HWEBLweIIb6vJdp1lgx293WlEPqBF3VcfdnTKJhf8en7Fax-qwBhvlf8WyLfp6q1UtOaP2lCEKmQHDmT25D7a0-xJH8oDeNmbDZYdbmsQaeHsf8pLjkK8QeojCppe8NmWIUwVhJHl2Gvbs6I3uEDz06XZvPlcSuCIxS7aE_LGMCekGDSq_Eg9AQ-U3ZZePCaKEgNWAasp0C3oAZzbye4lClIz92e36pDdGRzQ==', 'video_metadata': None}], 'role': 'model'}, 'citation_metadata': None, 'finish_message': None, 'token_count': None, 'finish_reason': 'STOP', 'avg_logprobs': None, 'grounding_metadata': None, 'index': 0, 'logprobs_result': None, 'safety_ratings': None, 'url_context_metadata': None}], 'create_time': None, 'model_version': 'gemini-3-pro-preview', 'prompt_feedback': None, 'response_id': 'hBNLaYbQFbafjrEPp-30wAs', 'usage_metadata': {'cache_tokens_details': None, 'cached_content_token_count': None, 'candidates_token_count': 7, 'candidates_tokens_details': None, 'prompt_token_count': 153, 'prompt_tokens_details': [{'modality': 'TEXT', 'token_count': 153}], 'thoughts_token_count': 815, 'tool_use_prompt_token_count': None, 'tool_use_prompt_tokens_details': None, 'total_token_count': 975, 'traffic_type': None}, 'automatic_function_calling_history': [], 'parsed': None}),\n",
       " ChatMessage(message={'role': 'user', 'content': 'How does the architecture of SLMs typically differ from LLMs? Keep it brief.'}, id='4f38905a-93d3-40cb-8b6d-fb069251b0f8', timestamp=datetime.datetime(2025, 12, 23, 22, 11, 16, 634999, tzinfo=datetime.timezone.utc), created_by='user', interop={}, metadata={}, provider_kwargs={}, original_response=None),\n",
       " ChatMessage(message={'summary': [{'text': '**Comparing SLM and LLM architectures**\\n\\nSLMs are smaller and optimized for efficiency, often using simpler or more efficient attention mechanisms to run on limited hardware. LLMs have larger parameter counts and deeper layers to capture more complex patterns and context with full self-attention.', 'type': 'summary_text'}], 'type': 'reasoning', 'encrypted_content': 'gAAAAABpSxOJfCUO1uEIAuLn8p0kV0EX13ixrgXAc6SFuWayBpYuHGruqj4MngqItPCwNWK-bGlcBEyp-DOjNJNHAPK7BYIu-_c12ojPRwOgJRM2GAIS3e4FHJfZDebd0bXZt3P-IX_quTuOEcU0WRzuLEZTKwpYZiHkoo26XGVaNxeh-nVoBjpMG2OU9URx5Ug8fOo-OiPun0Z2Ik8gr9n7IWiPqqV81fPSr_Bwf0iYQqKjJhPRIGTNKoT8vGVmcPGAOT_xgAdTsJ6ICI1BnQEHdAp_X0fTHYGQdklv146logUWhIsUOvx3ilCqx2BemD80W54thb6T0wK2SKYRfay5Xef-MP7IFmow3JiMJfkzuKjTXuMWhS6npT_7b-IBCd-smvOAT6LvsJojRccpcaXg_yAzZuBY1DpYvW-y6djUe82vZIHntFE8rbNipDAwHpRheUxLYxTnKQty0kjsM89fymRV9aRrQ6axAV4Z_-MqIwWFe07tWXy9irx70ygd1djkg8KpC8wEvYkRX-r5agosFpSft4_b9ILM8jtYZ_F1j9RaRK4d1w2a8HaV9YPJKKpUvK1jFFiDPWO1XZJp2ZKbfUro5qsDezr4Xb_0kT9QiEoQq0Gjp-3WBhikWiiyt-ynn1QMROQapDAewVrrD0whDpKa3JleGlAqCmeSqIymuNnK804oSAnRbmFvNul08pdtHrVo2Mvjn24O9dbv8UNEI_ba1xg11EcgGcE9AwyXP37zHhVut4Z4pZEsjGtXrPvNL3ZSWbOe_sWMNUglVF448DNnIZghlzwSWN-bGoJw0g1fO0YLUfn24ZfD_QUpkq7_pW8tMxkEjq332rIDq3JMe8fZNTQPLrJVugXjA16tPXURdAm-6uXhfvKgqqimO7lx3aCyyVlVMk4bNw9rVUh0CCV4t_6z-7KIkLzYC0zaNEujVI-7QfZphHgBrrLwzvhJt8lrSqJSdrBcyGvC8D861U_UMx4qJC02HUnqYp9S228LBGpiTteaGIRZKVqPaDElEU0Nz60_MQVHnm5gsIOI4mkxZjR07b3jy1l_w5SB8WcevoxDYVMhhHd2RU1Hhws9i9FyqhmL2ECxp0NXCsBbgwjaxBXYK3bV1Bbk-QdIyvz2QlOQaSnq_oFtHYhnqLwJRAL86eTGxPtRZ9j6ZYTdAjr-YnBOL9zNWZi6TvamYer2ocnNNfD_LaIMTTy5uO1gWv2iTk6dqbORdgwoPYICT8vNjsgOel3NpwYhA5t1rPk7XCoNMDkwouCjXVuFlUpXD-NS50P-ecqfnxA6mkpAGaNZ6WdenENyuupI6znkV1LfmIe0SMG6PW-Q522icU2Qyop3F8YOTy8xnQ_YFutqgGlYRQhR2jeQ6WBXupax5LM-JpKF31y0oCX8XKp1wzbvQyCyBhH3Ya-TCf2I_2QDOrTh98utFnR3K2-gjwhPFInse3K7T-8Kej7dzzFKTqWWlSHpjVunuZhrjsYqGMagpkIluovQjvqq8W0BwhyrSl1lS7vhFZsm84p6Toj_aP-nB8b1O2mPvKd6YO90TikLWmTu8NUtYfPs6Qt8K5rhEgUh7tF635Mf2NhFQp0VbAemwOaMJzmLttJ78zvKMSjNPqxPojfnx_zeitMpGbk1yYTfpmrq2J03CHH6QSLY3z4vl2XlaNcW_vZfB_SnUYd3zR0Brq6ptUHnxHIiubGF_iWm2HwkuFgxcIC8afMxO2I52tP7j8pjupBaey4sL67057k49FLvrecc4RA8Zo2KgvfEjV2uNcXRDk07ZVm4lFkOGBrf2OR7lCZVJpHzXYMijcHJm3_SqAKZLx8taafZwTivibM09k1EihzaHfi0icy8KCn_-XL8TV5c2U2ygr9BL8h0E6NJxm-pF5YUN7vrP8YpzqDZ0n1GrnqSCjlyyWYZ3kOWARMqVixXm_Rsw1Kg39dgN71cNPC37JC-dpaOpPyIz6KDUY2nDVpCZasqX5eLYJYqh6JzKwhe2gAldBIJQb_87PLEQn_jWTvtagEVs2drO_ckOOcic6p7hZ1dT_KuEfkG3Il_d0PxeOye1UfOhMGTG1F2616ierCnZWXoa1MgaqUSLQFG9MEeghqGEZv3uBZo9Ho5SIEtm5mwaLgjMAENSoJpaPFp8j_gpCNZ-_JjnVYCYH7DzSowXN2kC52TM9GiL1fV8pbqIJO6GHmeByfI5P-kEDNxsfPL9wH2EqhIt5_oh-gnMg9AGtVZ2JhM1jfWNmWZkeqYY19dQYDSM_SwB9AABHvi9ISO4xDzCC-DAKOgEjMiB4M_9GwLWbli8lgJGHanFvo7VpF1VbBAbYh8AoeOHUWMtTTOTq7gQKeKIUaRVypG9zeSrGFeiAJQ2yHg5j3mH7np6D0ygQ-W94yEPQG9qbj3mlZ1Szum3K4tN6_gueVCYjX_bo3lpuQlTrykz2q7Y0Cuxkc3DehQDP7GtNZgERbeMRzb9IOrT1RosZSfmm2vGFjG2IvNegJcEQymgslQWBf4UB5799fmuRGfx-2BeoLgS-ihsFQJ-YAUqjnKWpI='}, id='95b59f48-a485-41c7-809e-21e97d7235da', timestamp=datetime.datetime(2025, 12, 23, 22, 11, 21, 507928, tzinfo=datetime.timezone.utc), created_by='openai', interop={}, metadata={}, provider_kwargs={}, original_response=None),\n",
       " ChatMessage(message={'content': [{'annotations': [], 'text': 'SLMs use fewer parameters and simplified or efficiency‑oriented mechanisms (like sparse or windowed attention) to run on limited hardware, whereas LLMs stack more layers and full self‑attention to maximize capacity and generality.', 'type': 'output_text', 'logprobs': []}], 'role': 'assistant', 'type': 'message'}, id='486cbda2-2a75-4367-9b30-d672172bcbfd', timestamp=datetime.datetime(2025, 12, 23, 22, 11, 21, 507945, tzinfo=datetime.timezone.utc), created_by='openai', interop={}, metadata={}, provider_kwargs={}, original_response={'id': 'resp_001e89db9c1479c401694b1384dfd8819590dd5d3a5c57ac62', 'created_at': 1766527876.0, 'error': None, 'incomplete_details': None, 'instructions': None, 'metadata': {}, 'model': 'gpt-5.1-codex-max', 'object': 'response', 'output': [{'id': 'rs_001e89db9c1479c401694b138546008195b3e3e91e12640047', 'summary': [{'text': '**Comparing SLM and LLM architectures**\\n\\nSLMs are smaller and optimized for efficiency, often using simpler or more efficient attention mechanisms to run on limited hardware. LLMs have larger parameter counts and deeper layers to capture more complex patterns and context with full self-attention.', 'type': 'summary_text'}], 'type': 'reasoning', 'content': None, 'encrypted_content': 'gAAAAABpSxOJfCUO1uEIAuLn8p0kV0EX13ixrgXAc6SFuWayBpYuHGruqj4MngqItPCwNWK-bGlcBEyp-DOjNJNHAPK7BYIu-_c12ojPRwOgJRM2GAIS3e4FHJfZDebd0bXZt3P-IX_quTuOEcU0WRzuLEZTKwpYZiHkoo26XGVaNxeh-nVoBjpMG2OU9URx5Ug8fOo-OiPun0Z2Ik8gr9n7IWiPqqV81fPSr_Bwf0iYQqKjJhPRIGTNKoT8vGVmcPGAOT_xgAdTsJ6ICI1BnQEHdAp_X0fTHYGQdklv146logUWhIsUOvx3ilCqx2BemD80W54thb6T0wK2SKYRfay5Xef-MP7IFmow3JiMJfkzuKjTXuMWhS6npT_7b-IBCd-smvOAT6LvsJojRccpcaXg_yAzZuBY1DpYvW-y6djUe82vZIHntFE8rbNipDAwHpRheUxLYxTnKQty0kjsM89fymRV9aRrQ6axAV4Z_-MqIwWFe07tWXy9irx70ygd1djkg8KpC8wEvYkRX-r5agosFpSft4_b9ILM8jtYZ_F1j9RaRK4d1w2a8HaV9YPJKKpUvK1jFFiDPWO1XZJp2ZKbfUro5qsDezr4Xb_0kT9QiEoQq0Gjp-3WBhikWiiyt-ynn1QMROQapDAewVrrD0whDpKa3JleGlAqCmeSqIymuNnK804oSAnRbmFvNul08pdtHrVo2Mvjn24O9dbv8UNEI_ba1xg11EcgGcE9AwyXP37zHhVut4Z4pZEsjGtXrPvNL3ZSWbOe_sWMNUglVF448DNnIZghlzwSWN-bGoJw0g1fO0YLUfn24ZfD_QUpkq7_pW8tMxkEjq332rIDq3JMe8fZNTQPLrJVugXjA16tPXURdAm-6uXhfvKgqqimO7lx3aCyyVlVMk4bNw9rVUh0CCV4t_6z-7KIkLzYC0zaNEujVI-7QfZphHgBrrLwzvhJt8lrSqJSdrBcyGvC8D861U_UMx4qJC02HUnqYp9S228LBGpiTteaGIRZKVqPaDElEU0Nz60_MQVHnm5gsIOI4mkxZjR07b3jy1l_w5SB8WcevoxDYVMhhHd2RU1Hhws9i9FyqhmL2ECxp0NXCsBbgwjaxBXYK3bV1Bbk-QdIyvz2QlOQaSnq_oFtHYhnqLwJRAL86eTGxPtRZ9j6ZYTdAjr-YnBOL9zNWZi6TvamYer2ocnNNfD_LaIMTTy5uO1gWv2iTk6dqbORdgwoPYICT8vNjsgOel3NpwYhA5t1rPk7XCoNMDkwouCjXVuFlUpXD-NS50P-ecqfnxA6mkpAGaNZ6WdenENyuupI6znkV1LfmIe0SMG6PW-Q522icU2Qyop3F8YOTy8xnQ_YFutqgGlYRQhR2jeQ6WBXupax5LM-JpKF31y0oCX8XKp1wzbvQyCyBhH3Ya-TCf2I_2QDOrTh98utFnR3K2-gjwhPFInse3K7T-8Kej7dzzFKTqWWlSHpjVunuZhrjsYqGMagpkIluovQjvqq8W0BwhyrSl1lS7vhFZsm84p6Toj_aP-nB8b1O2mPvKd6YO90TikLWmTu8NUtYfPs6Qt8K5rhEgUh7tF635Mf2NhFQp0VbAemwOaMJzmLttJ78zvKMSjNPqxPojfnx_zeitMpGbk1yYTfpmrq2J03CHH6QSLY3z4vl2XlaNcW_vZfB_SnUYd3zR0Brq6ptUHnxHIiubGF_iWm2HwkuFgxcIC8afMxO2I52tP7j8pjupBaey4sL67057k49FLvrecc4RA8Zo2KgvfEjV2uNcXRDk07ZVm4lFkOGBrf2OR7lCZVJpHzXYMijcHJm3_SqAKZLx8taafZwTivibM09k1EihzaHfi0icy8KCn_-XL8TV5c2U2ygr9BL8h0E6NJxm-pF5YUN7vrP8YpzqDZ0n1GrnqSCjlyyWYZ3kOWARMqVixXm_Rsw1Kg39dgN71cNPC37JC-dpaOpPyIz6KDUY2nDVpCZasqX5eLYJYqh6JzKwhe2gAldBIJQb_87PLEQn_jWTvtagEVs2drO_ckOOcic6p7hZ1dT_KuEfkG3Il_d0PxeOye1UfOhMGTG1F2616ierCnZWXoa1MgaqUSLQFG9MEeghqGEZv3uBZo9Ho5SIEtm5mwaLgjMAENSoJpaPFp8j_gpCNZ-_JjnVYCYH7DzSowXN2kC52TM9GiL1fV8pbqIJO6GHmeByfI5P-kEDNxsfPL9wH2EqhIt5_oh-gnMg9AGtVZ2JhM1jfWNmWZkeqYY19dQYDSM_SwB9AABHvi9ISO4xDzCC-DAKOgEjMiB4M_9GwLWbli8lgJGHanFvo7VpF1VbBAbYh8AoeOHUWMtTTOTq7gQKeKIUaRVypG9zeSrGFeiAJQ2yHg5j3mH7np6D0ygQ-W94yEPQG9qbj3mlZ1Szum3K4tN6_gueVCYjX_bo3lpuQlTrykz2q7Y0Cuxkc3DehQDP7GtNZgERbeMRzb9IOrT1RosZSfmm2vGFjG2IvNegJcEQymgslQWBf4UB5799fmuRGfx-2BeoLgS-ihsFQJ-YAUqjnKWpI=', 'status': None}, {'id': 'msg_001e89db9c1479c401694b1388e2e481958b9090ec1531ec2a', 'content': [{'annotations': [], 'text': 'SLMs use fewer parameters and simplified or efficiency‑oriented mechanisms (like sparse or windowed attention) to run on limited hardware, whereas LLMs stack more layers and full self‑attention to maximize capacity and generality.', 'type': 'output_text', 'logprobs': []}], 'role': 'assistant', 'status': 'completed', 'type': 'message'}], 'parallel_tool_calls': True, 'temperature': 1.0, 'tool_choice': 'auto', 'tools': [], 'top_p': 0.985, 'background': False, 'conversation': None, 'max_output_tokens': 120000, 'max_tool_calls': None, 'previous_response_id': None, 'prompt': None, 'prompt_cache_key': None, 'prompt_cache_retention': None, 'reasoning': {'effort': 'medium', 'generate_summary': None, 'summary': 'detailed'}, 'safety_identifier': None, 'service_tier': 'default', 'status': 'completed', 'text': {'format': {'type': 'text'}, 'verbosity': 'medium'}, 'top_logprobs': 0, 'truncation': 'disabled', 'usage': {'input_tokens': 191, 'input_tokens_details': {'cached_tokens': 0}, 'output_tokens': 308, 'output_tokens_details': {'reasoning_tokens': 256}, 'total_tokens': 499}, 'user': None, 'billing': {'payer': 'developer'}, 'completed_at': 1766527881, 'store': False})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the conversation from a file\n",
    "loaded_messages: list[ChatMessage] = []\n",
    "data = json.loads(chat_file.read_text())\n",
    "for item in data:\n",
    "    loaded_messages.append(ChatMessage.from_json(json.dumps(item)))\n",
    "loaded_messages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interop-router",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
